# -*- coding: utf-8 -*-
"""[NLP] - Extraia o sentimento das a√ß√µes das manchetes de not√≠cias.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q5XMD30jPODtsJTrypGXtOah_FEsHF_e

# Projeto de NLP: An√°lise de Sentimento em Manchetes de Not√≠cias: Insights de Investimento para A√ß√µes da Facebook (FB) e Tesla (TSLA)

 üéØ **Objetivo:** Utilizar **an√°lise de sentimento** nas **manchetes de not√≠cias** financeiras do FINVIZ.com para identificar tend√™ncias do mercado e analisar o desempenho das a√ß√µes, permitindo decis√µes de investimento mais informadas e potencialmente lucrativas.

üîó**Fonte:** Esse projeto foi proposto pela trilha de aprendizados do DataCamp para testar e treinar **conceitos de NLP** (natural language processing ou  processamento de linguagem natural).

üì∞  **Dados:** [FINVIZ](https://finviz.com/) tem uma lista de sites confi√°veis, e as manchetes desses sites tendem a ser mais consistentes em seu jarg√£o do que as de blogueiros independentes. Padr√µes textuais consistentes melhorar√£o a an√°lise de sentimento.

üî¢ **Passos:**

1. Lendo os arquivos HTML
2. Entendendo os arquivos HTML
3. Extraindo as manchetes
4. Analisando sentimentos.
5. Prevendo os sentmentos.
6. Analisando os sentimentos ao longo do tempo
7. Fazendo alguns ajustes
8. Sentimento em um √∫nico dia de negocia√ß√£o e a√ß√µes
9. Visualizando

## 1. Lendo os arquivos HTML

Vamos come√ßar lendo e processando as manchetes oriundas de um **webscraping de 5 momentos diferentes do site da FINVI**Z, os arquivos est√£o em  HTML localizados em uma pasta chamada datasets.

Utilizando a biblioteca BeautifulSoup para an√°lise de HTML, o c√≥digo itera sobre cada arquivo na pasta, abre o arquivo em modo de leitura e parseia seu conte√∫do. A partir desse conte√∫do, ele busca por uma tabela espec√≠fica identificada pelo id `news-table`.

Cada tabela encontrada √© ent√£o armazenada em um dicion√°rio, onde a chave √© o nome do arquivo e o valor √© o objeto da tabela. Isso permite uma organiza√ß√£o eficiente e acesso r√°pido √†s tabelas extra√≠das para an√°lises ou manipula√ß√µes subsequentes.
"""

# Importando bibliotecas
from bs4 import BeautifulSoup
import os

#Criando um dicion√°rio vazio para preencher com as informa√ß√µes
html_tables = {}

# Para cada tabela na pasta datasets...
for table_name in os.listdir('datasets'):
    # Este √© o caminho para o arquivo. Provavelmente deve ser alterado
    table_path = f'/content/datasets/{table_name}'
    # Abrindo como um arquivo Python em modo somente leitura
    table_file = open(table_path, 'r')
    # Lendo o conte√∫do do arquivo para 'html'
    html = BeautifulSoup(table_file)
    # Encontrando 'news-table' no BeautifulSoup e carregar em 'html_table'
    html_table = html.find(id='news-table')
    # Adicionando a tabela ao nosso dicion√°rio
    html_tables[table_name] = html_table

"""## 2. Entendendo os arquivos HTML


Pegamos a tabela que cont√©m os t√≠tulos do arquivo HTML de cada a√ß√£o, mas antes de come√ßarmos a analisar essas tabelas ainda mais, precisamos entender como os dados dessa tabela est√£o estruturados.
"""

# Lendos um dos dias de manchetes
tsla = html_tables['tsla_22sep.html']
# Obtenhendo todas as linhas da tabela <tr> no arquivo e armazenando em 'tesla_tr'
tsla_tr = tsla.findAll('tr')

# Para cada linha...
for i, table_row in enumerate(tsla_tr):
    # Lendo o texto do elemento 'a' e armazenando em 'link_text'
    link_text = table_row.a.get_text()
    # Lendo o texto do elemento <td> e armazenando em 'data_text'
    data_text = table_row.td.get_text()
    # Imprimindo a contagem
    print(f'N√∫mero do arquivo {i+1}:')
    # Imprimindo o conte√∫do de 'link_text' e 'data_text'
    print(link_text)
    print(data_text)
    # Encerraremos o loop depois de quatro linhas para evitar sobrecarregar o notebook, (nem mexe nisso aqui hihi)
    if i == 3:
        break

"""## 3. Extraindo as manchetes

Como vimos acima, os dados interessantes dentro de cada linha da tabela (`<tr>`) est√£o no texto dentro das tags `<td>` e `<a>`. Vamos agora analisar os dados de todas as tabelas e estruturar os dados.
"""

# Armazenando as not√≠cias analisadas em uma lista
parsed_news = []
# Iterando pelas not√≠cias
for file_name, news_table in html_tables.items():
    # Iterando por todas as tags <tr> em 'news_table'
    for x in news_table.findAll('tr'):
        # Lendo o texto da tag <tr> e armazene em 'text'
        text = x.get_text()
        # Dividindo o texto da tag <td> em uma lista
        date_scrape = x.td.text.split()
        # Separando a data e a hora das manchetes de forma adequada
        # Se o comprimento de 'date_scrape' for 1, carregue 'time' com o √∫nico elemento
        # Caso contr√°rio, carregue 'date' com o primeiro elemento e 'time' com o segundo
        if len(date_scrape) == 1:
            time = date_scrape[0]
        else:
            date = date_scrape[0]
            time = date_scrape[1]

        # Extraindo o ticker do nome do arquivo, obtendo a string at√© o primeiro '_'
        ticker = file_name.split("_")[0]
        # Adicionando ticker, data, hora e manchete como uma lista √† lista 'parsed_news'
        parsed_news.append([ticker, date, time, x.a.text])

parsed_news[5]

"""## 4. Analisando sentimentos.

Com uma base estruturada podemos partir para a analise de sentimentos das manchetes. **A an√°lise de sentimento √© altamente dependente do contexto.** Por exemplo, a frase "Isso √© t√£o viciante!" geralmente tem uma conota√ß√£o positiva se estamos falando de um videogame que voc√™ est√° jogando com amigos, mas pode ter uma conota√ß√£o negativa quando nos referimos a opioides.

√â importante considerar que selecionamos as manchetes para tentar capturar o sentimento dos jornalistas financeiros, que, assim como outros profissionais, possuem uma linguagem espec√≠fica. Agora, vamos ajustar o NLTK para refletir a perspectiva de um jornalista financeiro, adicionando novas palavras e valores de sentimento ao nosso l√©xico.


**Detalhamento do ajuste l√©xico:**
* **'crushes'**: 10: A palavra "crushes" (como em "domina" ou "arrasou") √© atribu√≠da um valor positivo alto (+10), indicando um sentimento muito positivo.
* **'beats'**: 5: A palavra "beats" (como em "vence" ou "supera") tem um valor positivo moderado (+5), sugerindo um sentimento positivo.
* **'misses'**: -5: A palavra "misses" (como em "perdeu" ou "sentiu falta") recebe um valor negativo leve (-5), indicando um sentimento levemente negativo.
* **'trouble'**: -10: A palavra "trouble" (como em "problema" ou "dificuldade") tem um valor negativo moderado (-10), sugerindo um sentimento mais negativo.
* **'falls'**: -100: A palavra "falls" (como em "desmorona" ou "cai") √© atribu√≠da um valor negativo muito alto (-100), refletindo um sentimento extremamente negativo.
"""

!pip install NLTK

import nltk
nltk.download('vader_lexicon')

# Importando NLTK VADER para an√°lise de sentimento
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Definindo novo l√©xico (novas palavras e valores)
new_words = {
    'crushes': 10,
    'beats': 5,
    'misses': -5,
    'trouble': -10,
    'falls': -100,
}

# Instanciando o analisador de intensidade de sentimento com o l√©xico existente
vader = SentimentIntensityAnalyzer()

# Atualizando o l√©xico
vader.lexicon.update(new_words)

"""## 5. Prevendo os sentmentos.


Agora que temos os dados e o algoritmo carregados, chegaremos ao cerne da quest√£o: prever o sentimento das manchetes! Felizmente para n√≥s, o VADER √© de n√≠vel muito alto, ent√£o, neste caso, n√£o ajustaremos o modelo al√©m das adi√ß√µes de l√©xico anteriores.
VADER ‚Äúpronto para uso‚Äù com algum l√©xico extra provavelmente se traduziria em pesadas perdas com dinheiro real. Uma verdadeira ferramenta de an√°lise de sentimento com chances de ser lucrativa exigir√° um l√©xico de not√≠cias muito extenso e dedicado ao financiamento. Al√©m disso, tamb√©m pode n√£o ser suficiente usar um modelo pr√©-treinado como o VADER.
"""

import pandas as pd
# Definindo os nomes da coluna
columns = ['ticker', 'date', 'time', 'headline']
# Convertendo a lista de manchetes em um DataFrame
scored_news = pd.DataFrame(parsed_news, columns=columns)

# Iterando pelas manchetes e obtenha os escores de polaridade
scores = [vader.polarity_scores(headline) for headline in scored_news.headline]
# Convertendo a lista de dicion√°rios em um DataFrame
scores_df = pd.DataFrame(scores)
scored_news.columns = columns
# Juntando os DataFrames
scored_news = scored_news.join(scores_df)
# Convertendo a coluna de data de string para data
scored_news['date'] = pd.to_datetime(scored_news.date).dt.date

scored_news.head()

"""## 6. Analisando os sentimentos ao longo do tempo

Agora que temos as pontua√ß√µes, vamos come√ßar a tra√ßar os resultados. Come√ßaremos tra√ßando a s√©rie temporal das a√ß√µes que temos (fb, tsla).
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
plt.style.use("fivethirtyeight")
# %matplotlib inline

# Agrupando por data e ticker nas colunas do scored_news e calcule a m√©dia (deu um problema no tipo, tivemos que filtrar)
mean_c = scored_news.groupby(['date', 'ticker'])[scored_news.select_dtypes(include='number').columns].mean()
# Desempilhando a coluna ticker
mean_c = mean_c.unstack('ticker')
# Obtendo a se√ß√£o transversal de 'compound' no eixo das colunas
mean_c = mean_c.xs("compound", axis="columns")
# Plote um gr√°fico de barras com pandas
mean_c.plot.bar(figsize = (10, 6));

"""## 7. Fazendo alguns ajustes

O que aconteceu com Tesla em 22 de novembro? Como temos as manchetes em nosso DataFrame, uma r√°pida olhada revela que h√° alguns problemas naquele dia espec√≠fico:

Existem apenas 5 manchetes para aquele dia.
Duas manchetes s√£o literalmente iguais a outra, mas de outro meio de comunica√ß√£o.
Vamos limpar um pouco o conjunto de dados, mas n√£o muito! Embora algumas manchetes sejam a mesma not√≠cia de fontes diferentes, o fato de serem escritas de forma diferente pode fornecer perspectivas diferentes sobre a mesma hist√≥ria. Al√©m disso, quando uma not√≠cia √© mais importante, ela tende a receber mais manchetes de diversas fontes. O que queremos eliminar s√£o as manchetes copiadas literalmente, j√° que muito provavelmente v√™m do mesmo jornalista e est√£o apenas sendo "encaminhadas", por assim dizer.
"""

# Contando o n√∫mero de manchetes em 'scored_news' (armazenado como um inteiro)
num_news_before = scored_news.headline.count()
# Removendo duplicatas com base em 'ticker' e 'headline'
scored_news_clean = scored_news.drop_duplicates(subset=['headline', 'ticker'])
# Contando o n√∫mero de manchetes ap√≥s remover duplicatas (armazenado como um inteiro)
num_news_after = scored_news_clean.headline.count()
# Imprimindo os n√∫meros antes e depois para ter uma ideia de como fizemos
f"Antes t√≠nhamos {num_news_before} manchetes, agora temos {num_news_after}"

"""## 8. Sentimento em um √∫nico dia de negocia√ß√£o e a√ß√µes
Apenas para entender as possibilidades desse conjunto de dados e ter uma ideia melhor dos dados, vamos nos concentrar em um dia de negocia√ß√£o e em uma √∫nica a√ß√£o. Faremos um gr√°fico informativo onde veremos o menor gr√£o poss√≠vel: t√≠tulo e subpartituras.
"""

# Defina o √≠ndice como 'ticker' e 'date'
single_day = scored_news_clean.set_index(['ticker', 'date'])
# Obtenha a se√ß√£o transversal da linha 'fb'
single_day = single_day.xs('fb')

# Converta o √≠ndice de data para datetime
single_day.index = pd.to_datetime(single_day.index)

# Selecione o dia 3 de janeiro de 2019
single_day = single_day.loc['2019-01-03']
# Converta a string de data e hora para apenas o hor√°rio
single_day['time'] = pd.to_datetime(single_day['time']).dt.time
# Defina o √≠ndice como 'time' e ordene por ele
single_day = single_day.set_index('time')
# Ordene
single_day = single_day.sort_index()

single_day.head()

"""## 9 Visualizando
Faremos um gr√°fico para visualizar as pontua√ß√µes positivas, negativas e neutras para um √∫nico dia de negocia√ß√£o e uma √∫nica a√ß√£o.
"""

# T√≠tulo e cores para o gr√°fico
TITLE = "Sentimento negativo, neutro e positivo para FB em 2019-01-03"
COLORS = ["red","orange", "green"]

# Remova as colunas que n√£o s√£o √∫teis para o gr√°fico
plot_day = single_day.drop(['compound', 'headline'], axis=1)

# Altere os nomes das colunas para 'negativo', 'neutro' e 'positivo'
plot_day.columns = ['negativo', 'neutro', 'positivo']

# Plote um gr√°fico de barras empilhadas
plot_day.plot.bar(stacked=True, figsize=(10, 6), title=TITLE, color=COLORS).legend(bbox_to_anchor=(1.2, 0.5))
plt.ylabel("escores")
plt.show()

"""O impacto das not√≠cias sobre uma a√ß√£o, como o Facebook, esse gr√°fico pode mostrar como o sentimento das not√≠cias mudou ao longo do dia e ajudar a correlacionar esses sentimentos com eventos espec√≠ficos ou mudan√ßas no mercado, mostrando como esses sentimentos variam ao longo do dia, ajudando a identificar padr√µes e poss√≠veis influ√™ncias das not√≠cias no sentimento geral."""

# Verificando a distribui√ß√£o das pontua√ß√µes de sentimento para todas as not√≠cias.
plt.figure(figsize=(12, 6))
plt.hist(scored_news['compound'], bins=30, edgecolor='black')
plt.xlabel('Pontua√ß√£o de Sentimento')
plt.ylabel('N√∫mero de Manchetes')
plt.title('Distribui√ß√£o das Pontua√ß√µes de Sentimento')
plt.show()

import seaborn as sns
#Visualize a intensidade m√©dia dos sentimentos por dia e por ticker.
# Criar uma tabela de conting√™ncia para o mapa de calor
heatmap_data = scored_news.pivot_table(index='date', columns='ticker', values='compound', aggfunc='mean')

# Plotar o mapa de calor
plt.figure(figsize=(12, 8))
sns.heatmap(heatmap_data, cmap='coolwarm', annot=True, fmt='.1f')
plt.title('Mapa de Calor das Pontua√ß√µes de Sentimento por Data e Ticker')
plt.show()

single_day.head()

# Calcular a soma dos sentimentos para o dia selecionado
sentiment_counts = single_day[['neg', 'neu', 'pos']].sum()

# Plotar o gr√°fico de pizza
plt.figure(figsize=(8, 8))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', colors=['red', 'orange', 'green'])
plt.title('Distribui√ß√£o Percentual dos Sentimentos em FB em 2019-01-03')
plt.show()